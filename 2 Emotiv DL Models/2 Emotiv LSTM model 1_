{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Ma1h3Y1DoSl2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713888253408,"user_tz":240,"elapsed":2146,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"e389d743-5316-4291-b823-678069bc0d18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os"],"metadata":{"id":"r4AOB6Zvoaug","executionInfo":{"status":"ok","timestamp":1713888254113,"user_tz":240,"elapsed":708,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Load Data"],"metadata":{"id":"ahdQtqI4qVqC"}},{"cell_type":"code","source":["eeg_base_path = \"/content/drive/MyDrive/JHU/MLMA/MLMA Project/BS-HMS-Dataset/Emotiv EEG\"\n","csv_files = [f for f in os.listdir(eeg_base_path) if not any(test in f for test in ['Test20', 'Test21', 'Test27'])]\n","\n","eeg_array = []\n","\n","# Define a fixed number of timesteps for each sample\n","num_timesteps = 128\n","\n","for filename in csv_files:\n","    file_path = os.path.join(eeg_base_path, filename)\n","    df = pd.read_csv(file_path)\n","    # Select the relevant EEG channels\n","    df = df[[ \"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]]\n","    # Truncate or pad the DataFrame to have a fixed number of timesteps\n","    if len(df) > num_timesteps:\n","        # Truncate the excess rows\n","        df = df.iloc[:num_timesteps]\n","    elif len(df) < num_timesteps:\n","        # Pad the DataFrame with rows of zeros\n","        padding = pd.DataFrame(np.zeros((num_timesteps - len(df), len(df.columns))), columns=df.columns)\n","        df = pd.concat([df, padding])\n","    # Convert the DataFrame to a 2D array and append to the list\n","    eeg_array.append(df.values)\n","\n","# Stack the 2D arrays into a 3D tensor\n","eeg_tensor = np.stack(eeg_array, axis=0)\n","\n","# Now 'eeg_tensor' is a 3D tensor of shape (samples, timesteps, features)\n","eeg_tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmAJZtpPozOP","executionInfo":{"status":"ok","timestamp":1713888310485,"user_tz":240,"elapsed":56375,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"15ffe6de-6938-42c1-eb96-db246eb7a81f"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200, 128, 14)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Train Test Split"],"metadata":{"id":"NscnVTuzqdG3"}},{"cell_type":"code","source":[],"metadata":{"id":"EecWocG9qZ7g","executionInfo":{"status":"ok","timestamp":1713888310488,"user_tz":240,"elapsed":15,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Assuming `eeg_tensor` is your data tensor and `labels` is an array of corresponding labels\n","# Define groups\n","group_size = 8  # Size of each group\n","num_samples = eeg_tensor.shape[0]\n","groups = np.arange(num_samples) // group_size  # Integer division to assign group numbers\n","labels=np.array([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n","        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n","        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1])\n","\n","\n","# Split groups into training and testing sets\n","train_groups, test_groups = train_test_split(np.unique(groups), test_size=0.2, random_state=42)\n","\n","# Use boolean indexing to create the actual training and testing sets based on groups\n","X_train = eeg_tensor[np.isin(groups, train_groups)]\n","y_train = labels[np.isin(groups, train_groups)]\n","X_test = eeg_tensor[np.isin(groups, test_groups)]\n","y_test = labels[np.isin(groups, test_groups)]\n","\n","# Now you have your grouped training and testing sets\n","print(\"Training set shape:\", X_train.shape)\n","print(\"Testing set shape:\", X_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Eb_dlxfqgA3","executionInfo":{"status":"ok","timestamp":1713888311117,"user_tz":240,"elapsed":642,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"f76bd6ff-7c0b-4ae4-a423-9ea91d2068d9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape: (160, 128, 14)\n","Testing set shape: (40, 128, 14)\n"]}]},{"cell_type":"code","source":["# Keep splitting Training data to get validation data\n","import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split, GroupKFold\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import LambdaCallback\n","import tensorflow as tf\n","\n","# Correct one-hot encoding\n","y = to_categorical(labels, num_classes=np.unique(labels).size)\n","\n","# Define groups and split them\n","group_size = 8\n","num_samples = eeg_tensor.shape[0]\n","groups = np.arange(num_samples) // group_size\n","train_group_ids, test_group_ids = train_test_split(np.unique(groups), test_size=0.2, random_state=42)\n","\n","# Filtering indices for training and testing based on group IDs\n","train_index = np.isin(groups, train_group_ids)\n","test_index = np.isin(groups, test_group_ids)\n","\n","# Applying indices to split data\n","X_train = eeg_tensor[train_index]\n","y_train = y[train_index]\n","X_test = eeg_tensor[test_index]\n","y_test = y[test_index]\n","train_groups = groups[train_index]\n","\n","# Further split X_train and y_train for internal validation\n","# First, get the group identifiers for the new train split\n","unique_train_groups = np.unique(train_groups)\n","train_cv_group_ids, val_cv_group_ids = train_test_split(unique_train_groups, test_size=0.1, random_state=42)  # 90% train, 10% validation\n","\n","# Now split based on groups again\n","train_cv_index = np.isin(train_groups, train_cv_group_ids)\n","val_cv_index = np.isin(train_groups, val_cv_group_ids)\n","\n","X_train_cv = X_train[train_cv_index]\n","y_train_cv = y_train[train_cv_index]\n","X_val_cv = X_train[val_cv_index]\n","y_val_cv = y_train[val_cv_index]\n"],"metadata":{"id":"kznY5wukkBhz","executionInfo":{"status":"ok","timestamp":1713888314682,"user_tz":240,"elapsed":3566,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## LSTM + CV"],"metadata":{"id":"TIOcTiZKqhNa"}},{"cell_type":"markdown","source":["### Basic Model"],"metadata":{"id":"djC_drX_r_8F"}},{"cell_type":"code","source":["# Model creation function\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        LSTM(64, return_sequences=True, input_shape=input_shape),\n","        Dropout(0.5),\n","        LSTM(32),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"nGb3M7h3pbnO","executionInfo":{"status":"ok","timestamp":1713888314683,"user_tz":240,"elapsed":32,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a64fVg4vmHx8","executionInfo":{"status":"ok","timestamp":1713888377005,"user_tz":240,"elapsed":62352,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"9e829092-a95e-4674-8c6c-bdf8b6f8cd74"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.7111, Accuracy = 0.5268, Val Loss = 0.7004, Val Accuracy = 0.3750\n","Epoch 40: Loss = 0.7035, Accuracy = 0.5179, Val Loss = 0.7081, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.7091, Accuracy = 0.5179, Val Loss = 0.7049, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6935, Accuracy = 0.5357, Val Loss = 0.7062, Val Accuracy = 0.3750\n","Epoch 100: Loss = 0.6985, Accuracy = 0.5357, Val Loss = 0.7108, Val Accuracy = 0.3750\n","Fold accuracy: 37.50%\n","Epoch 20: Loss = 0.6871, Accuracy = 0.5446, Val Loss = 0.6950, Val Accuracy = 0.5625\n","Epoch 40: Loss = 0.7017, Accuracy = 0.4643, Val Loss = 0.6970, Val Accuracy = 0.4062\n","Epoch 60: Loss = 0.6989, Accuracy = 0.5089, Val Loss = 0.6965, Val Accuracy = 0.5625\n","Epoch 80: Loss = 0.6871, Accuracy = 0.5446, Val Loss = 0.6923, Val Accuracy = 0.5625\n","Epoch 100: Loss = 0.6991, Accuracy = 0.4911, Val Loss = 0.6960, Val Accuracy = 0.4375\n","Fold accuracy: 43.75%\n","Epoch 20: Loss = 0.7358, Accuracy = 0.3929, Val Loss = 0.6937, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7147, Accuracy = 0.5089, Val Loss = 0.6936, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6974, Accuracy = 0.5179, Val Loss = 0.6936, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6884, Accuracy = 0.5714, Val Loss = 0.6945, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6967, Accuracy = 0.4554, Val Loss = 0.6937, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6971, Accuracy = 0.5667, Val Loss = 0.6971, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6991, Accuracy = 0.4917, Val Loss = 0.6971, Val Accuracy = 0.4583\n","Epoch 60: Loss = 0.7066, Accuracy = 0.5000, Val Loss = 0.6981, Val Accuracy = 0.4583\n","Epoch 80: Loss = 0.7142, Accuracy = 0.3917, Val Loss = 0.6969, Val Accuracy = 0.4583\n","Epoch 100: Loss = 0.6917, Accuracy = 0.5500, Val Loss = 0.6946, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6975, Accuracy = 0.5500, Val Loss = 0.7462, Val Accuracy = 0.3333\n","Epoch 40: Loss = 0.7220, Accuracy = 0.5000, Val Loss = 0.7338, Val Accuracy = 0.3333\n","Epoch 60: Loss = 0.6828, Accuracy = 0.5917, Val Loss = 0.7295, Val Accuracy = 0.3333\n","Epoch 80: Loss = 0.6943, Accuracy = 0.5417, Val Loss = 0.7342, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.6965, Accuracy = 0.5417, Val Loss = 0.7266, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 42.92% ± 6.67%\n"]}]},{"cell_type":"markdown","source":["### Finetune with Bidirection"],"metadata":{"id":"DHXUe65gq_eQ"}},{"cell_type":"code","source":["# Model creation function\n","from tensorflow.keras.layers import Bidirectional\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),  # Increased and bidirectional\n","        Dropout(0.5),\n","        LSTM(32),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"3nrVkRrXhd0i","executionInfo":{"status":"ok","timestamp":1713888377005,"user_tz":240,"elapsed":22,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKZipQsWiXMH","executionInfo":{"status":"ok","timestamp":1713888450596,"user_tz":240,"elapsed":73611,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"e11fa809-7521-4ace-b076-023e54f052ab"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.6890, Accuracy = 0.4911, Val Loss = 0.7050, Val Accuracy = 0.3750\n","Epoch 40: Loss = 0.6932, Accuracy = 0.5625, Val Loss = 0.7045, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.7049, Accuracy = 0.4732, Val Loss = 0.7075, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6803, Accuracy = 0.5893, Val Loss = 0.7029, Val Accuracy = 0.3750\n","Epoch 100: Loss = 0.7120, Accuracy = 0.4107, Val Loss = 0.7041, Val Accuracy = 0.3750\n","Fold accuracy: 37.50%\n","Epoch 20: Loss = 0.7021, Accuracy = 0.5357, Val Loss = 0.6879, Val Accuracy = 0.5625\n","Epoch 40: Loss = 0.7175, Accuracy = 0.4464, Val Loss = 0.6914, Val Accuracy = 0.5625\n","Epoch 60: Loss = 0.6876, Accuracy = 0.5804, Val Loss = 0.6914, Val Accuracy = 0.5625\n","Epoch 80: Loss = 0.7007, Accuracy = 0.4464, Val Loss = 0.6906, Val Accuracy = 0.5625\n","Epoch 100: Loss = 0.6877, Accuracy = 0.5357, Val Loss = 0.6902, Val Accuracy = 0.5625\n","Fold accuracy: 56.25%\n","Epoch 20: Loss = 0.6907, Accuracy = 0.5179, Val Loss = 0.6944, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7099, Accuracy = 0.5179, Val Loss = 0.6941, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.7013, Accuracy = 0.5000, Val Loss = 0.6939, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6851, Accuracy = 0.5714, Val Loss = 0.6935, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.7016, Accuracy = 0.4732, Val Loss = 0.6933, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.7355, Accuracy = 0.4583, Val Loss = 0.6956, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7095, Accuracy = 0.4333, Val Loss = 0.6940, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6805, Accuracy = 0.5333, Val Loss = 0.6960, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.7046, Accuracy = 0.4500, Val Loss = 0.6950, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6947, Accuracy = 0.4667, Val Loss = 0.6939, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.7189, Accuracy = 0.4667, Val Loss = 0.7338, Val Accuracy = 0.3333\n","Epoch 40: Loss = 0.6816, Accuracy = 0.5667, Val Loss = 0.7387, Val Accuracy = 0.3333\n","Epoch 60: Loss = 0.6883, Accuracy = 0.5583, Val Loss = 0.7368, Val Accuracy = 0.3333\n","Epoch 80: Loss = 0.7109, Accuracy = 0.5167, Val Loss = 0.7425, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.7088, Accuracy = 0.5250, Val Loss = 0.7289, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 45.42% ± 8.58%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J8W5ghpVjH-L","executionInfo":{"status":"ok","timestamp":1713888450597,"user_tz":240,"elapsed":7,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Fintune with Dropout"],"metadata":{"id":"h-y1clOXrLmI"}},{"cell_type":"code","source":["\n","# Model creation function\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        LSTM(64, return_sequences=True, input_shape=input_shape),\n","        Dropout(0.1),\n","        LSTM(32),\n","        Dropout(0.1),\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"BZQkwtvKp9Av","executionInfo":{"status":"ok","timestamp":1713888450597,"user_tz":240,"elapsed":5,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_mWS0HgqQbj","executionInfo":{"status":"ok","timestamp":1713888512978,"user_tz":240,"elapsed":62386,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"d6a86936-44b5-428c-a6a8-55d7ab559f7a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.6860, Accuracy = 0.5625, Val Loss = 0.6912, Val Accuracy = 0.6562\n","Epoch 40: Loss = 0.6833, Accuracy = 0.5000, Val Loss = 0.6950, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.6905, Accuracy = 0.4018, Val Loss = 0.6991, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6987, Accuracy = 0.4911, Val Loss = 0.6895, Val Accuracy = 0.6250\n","Epoch 100: Loss = 0.6974, Accuracy = 0.4643, Val Loss = 0.6971, Val Accuracy = 0.3750\n","Fold accuracy: 37.50%\n","Epoch 20: Loss = 0.7007, Accuracy = 0.4286, Val Loss = 0.6904, Val Accuracy = 0.5625\n","Epoch 40: Loss = 0.7029, Accuracy = 0.4375, Val Loss = 0.6949, Val Accuracy = 0.4375\n","Epoch 60: Loss = 0.6925, Accuracy = 0.4911, Val Loss = 0.6960, Val Accuracy = 0.4375\n","Epoch 80: Loss = 0.6908, Accuracy = 0.5089, Val Loss = 0.6912, Val Accuracy = 0.5625\n","Epoch 100: Loss = 0.6936, Accuracy = 0.4732, Val Loss = 0.6946, Val Accuracy = 0.4375\n","Fold accuracy: 43.75%\n","Epoch 20: Loss = 0.6893, Accuracy = 0.5179, Val Loss = 0.6939, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6943, Accuracy = 0.5000, Val Loss = 0.6936, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6898, Accuracy = 0.5000, Val Loss = 0.6931, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6980, Accuracy = 0.5179, Val Loss = 0.6941, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6962, Accuracy = 0.5000, Val Loss = 0.6945, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6874, Accuracy = 0.5333, Val Loss = 0.6951, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6986, Accuracy = 0.4750, Val Loss = 0.6902, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6915, Accuracy = 0.4917, Val Loss = 0.6911, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6958, Accuracy = 0.4500, Val Loss = 0.6912, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6853, Accuracy = 0.5583, Val Loss = 0.6927, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6930, Accuracy = 0.5667, Val Loss = 0.7356, Val Accuracy = 0.3333\n","Epoch 40: Loss = 0.6921, Accuracy = 0.5667, Val Loss = 0.7323, Val Accuracy = 0.3333\n","Epoch 60: Loss = 0.6954, Accuracy = 0.5250, Val Loss = 0.7245, Val Accuracy = 0.3333\n","Epoch 80: Loss = 0.6954, Accuracy = 0.5333, Val Loss = 0.7429, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.6911, Accuracy = 0.5500, Val Loss = 0.7343, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 42.92% ± 6.67%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iAPIwSQJqQzd","executionInfo":{"status":"ok","timestamp":1713888512979,"user_tz":240,"elapsed":27,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Fintune with Neurons"],"metadata":{"id":"i39gazgSrlnu"}},{"cell_type":"code","source":["\n","# Model creation function\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        LSTM(256, return_sequences=True, input_shape=input_shape),\n","        Dropout(0.1),\n","        LSTM(128),\n","        Dropout(0.1),\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"fJJaDdUQrlYr","executionInfo":{"status":"ok","timestamp":1713888512979,"user_tz":240,"elapsed":25,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ut3rnWi3rsl-","executionInfo":{"status":"ok","timestamp":1713888577659,"user_tz":240,"elapsed":64704,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"3b7f8699-15cc-4ca0-9d75-d8a8da62c310"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.7153, Accuracy = 0.4464, Val Loss = 0.6783, Val Accuracy = 0.6250\n","Epoch 40: Loss = 0.6889, Accuracy = 0.5357, Val Loss = 0.6972, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.6911, Accuracy = 0.5357, Val Loss = 0.7186, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6928, Accuracy = 0.5089, Val Loss = 0.7148, Val Accuracy = 0.3750\n","Epoch 100: Loss = 0.6939, Accuracy = 0.4911, Val Loss = 0.7035, Val Accuracy = 0.3750\n","Fold accuracy: 37.50%\n","Epoch 20: Loss = 0.6973, Accuracy = 0.5089, Val Loss = 0.7024, Val Accuracy = 0.4375\n","Epoch 40: Loss = 0.6937, Accuracy = 0.5446, Val Loss = 0.6880, Val Accuracy = 0.5625\n","Epoch 60: Loss = 0.7067, Accuracy = 0.4286, Val Loss = 0.6901, Val Accuracy = 0.5625\n","Epoch 80: Loss = 0.6984, Accuracy = 0.4643, Val Loss = 0.6971, Val Accuracy = 0.4375\n","Epoch 100: Loss = 0.7104, Accuracy = 0.4554, Val Loss = 0.7032, Val Accuracy = 0.4375\n","Fold accuracy: 43.75%\n","Epoch 20: Loss = 0.6905, Accuracy = 0.5268, Val Loss = 0.6941, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7014, Accuracy = 0.5089, Val Loss = 0.6932, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6866, Accuracy = 0.5089, Val Loss = 0.6958, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6832, Accuracy = 0.5714, Val Loss = 0.6951, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6881, Accuracy = 0.5179, Val Loss = 0.6942, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6934, Accuracy = 0.5250, Val Loss = 0.6953, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6898, Accuracy = 0.5500, Val Loss = 0.6951, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6860, Accuracy = 0.5500, Val Loss = 0.6950, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6945, Accuracy = 0.4417, Val Loss = 0.6935, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6927, Accuracy = 0.5167, Val Loss = 0.6937, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6940, Accuracy = 0.5333, Val Loss = 0.7013, Val Accuracy = 0.3333\n","Epoch 40: Loss = 0.7059, Accuracy = 0.5417, Val Loss = 0.7518, Val Accuracy = 0.3333\n","Epoch 60: Loss = 0.6877, Accuracy = 0.5417, Val Loss = 0.7194, Val Accuracy = 0.3333\n","Epoch 80: Loss = 0.6955, Accuracy = 0.5333, Val Loss = 0.7220, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.6839, Accuracy = 0.5500, Val Loss = 0.7549, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 42.92% ± 6.67%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WbcxugCMrt-j","executionInfo":{"status":"ok","timestamp":1713888577659,"user_tz":240,"elapsed":4,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Fintune with layers"],"metadata":{"id":"ZHEgeDLLsLk3"}},{"cell_type":"code","source":["def create_lstm_model(input_shape,num_classes):\n","    model = Sequential([\n","        # First LSTM layer with dropout\n","        LSTM(256, return_sequences=True, input_shape=input_shape),\n","        Dropout(0.1),\n","        # Second LSTM layer with dropout\n","        LSTM(128, return_sequences=True),\n","        Dropout(0.1),\n","        # Additional LSTM layer\n","        LSTM(64),\n","        Dropout(0.1),\n","        # Dense layer for feature interpretation\n","        Dense(32, activation='relu'),\n","        Dropout(0.1),\n","        # Output layer with a single neuron and sigmoid activation for binary classification\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"RcXvsmGasN66","executionInfo":{"status":"ok","timestamp":1713888824965,"user_tz":240,"elapsed":556,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YarBZOulsxnV","executionInfo":{"status":"ok","timestamp":1713888917181,"user_tz":240,"elapsed":91229,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"2c8ac008-6ef3-43d1-ba2e-f335671af51f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.6970, Accuracy = 0.5000, Val Loss = 0.6992, Val Accuracy = 0.3750\n","Epoch 40: Loss = 0.6929, Accuracy = 0.5357, Val Loss = 0.6997, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.6941, Accuracy = 0.4732, Val Loss = 0.6961, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6940, Accuracy = 0.5357, Val Loss = 0.7057, Val Accuracy = 0.3750\n","Epoch 100: Loss = 0.6919, Accuracy = 0.5089, Val Loss = 0.7039, Val Accuracy = 0.3750\n","Fold accuracy: 37.50%\n","Epoch 20: Loss = 0.6976, Accuracy = 0.4643, Val Loss = 0.6979, Val Accuracy = 0.4375\n","Epoch 40: Loss = 0.6988, Accuracy = 0.4554, Val Loss = 0.6969, Val Accuracy = 0.4375\n","Epoch 60: Loss = 0.6927, Accuracy = 0.5357, Val Loss = 0.6955, Val Accuracy = 0.4375\n","Epoch 80: Loss = 0.6956, Accuracy = 0.4196, Val Loss = 0.6919, Val Accuracy = 0.5625\n","Epoch 100: Loss = 0.6940, Accuracy = 0.5268, Val Loss = 0.6925, Val Accuracy = 0.5625\n","Fold accuracy: 56.25%\n","Epoch 20: Loss = 0.6947, Accuracy = 0.5000, Val Loss = 0.6940, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6873, Accuracy = 0.5625, Val Loss = 0.6933, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6866, Accuracy = 0.5714, Val Loss = 0.6937, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6882, Accuracy = 0.4911, Val Loss = 0.6935, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6947, Accuracy = 0.4821, Val Loss = 0.6934, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.7028, Accuracy = 0.5000, Val Loss = 0.6953, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.6968, Accuracy = 0.4833, Val Loss = 0.6947, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.6954, Accuracy = 0.4750, Val Loss = 0.6935, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6923, Accuracy = 0.5167, Val Loss = 0.6943, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6968, Accuracy = 0.5083, Val Loss = 0.6946, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6832, Accuracy = 0.5500, Val Loss = 0.7366, Val Accuracy = 0.3333\n","Epoch 40: Loss = 0.6930, Accuracy = 0.5583, Val Loss = 0.7358, Val Accuracy = 0.3333\n","Epoch 60: Loss = 0.6900, Accuracy = 0.5500, Val Loss = 0.7335, Val Accuracy = 0.3333\n","Epoch 80: Loss = 0.6881, Accuracy = 0.5500, Val Loss = 0.7342, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.6897, Accuracy = 0.5500, Val Loss = 0.7336, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 45.42% ± 8.58%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DkV_xUGAurPS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finetune by complexity"],"metadata":{"id":"HjVFbJoBv9be"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        # First Bidirectional LSTM layer with dropout\n","        Bidirectional(LSTM(256, return_sequences=True, input_shape=input_shape)),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        # Second Bidirectional LSTM layer with dropout\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        # Third LSTM layer with dropout, not returning sequences to flatten the output\n","        LSTM(64),\n","        BatchNormalization(),\n","        Dropout(0.2),\n","\n","        # Additional Dense layers for deeper feature interpretation\n","        Dense(64, activation='relu'),\n","        Dropout(0.2),\n","        Dense(32, activation='relu'),\n","        Dropout(0.2),\n","\n","        # Output layer with a single neuron and sigmoid activation for binary classification\n","        Dense(num_classes, activation='sigmoid')\n","    ])\n","\n","    # Compile the model with appropriate loss function for binary classification\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"c-R2eLzgvrhW","executionInfo":{"status":"ok","timestamp":1713889036368,"user_tz":240,"elapsed":259,"user":{"displayName":"Wang","userId":"06822780026042244845"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\n","from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n","cv_scores=[]\n","best_scores=0\n","best_model=None\n","# Define the callback for reporting loss and accuracy every 20 epochs\n","report_callback = LambdaCallback(\n","    on_epoch_end=lambda epoch, logs:\n","        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Val Loss = {logs['val_loss']:.4f}, Val Accuracy = {logs['val_accuracy']:.4f}\")\n","        if (epoch + 1) % 20 == 0 else None\n",")\n","\n","# Define the LSTM model input shape and number of classes\n","input_shape = (128, 14)\n","num_classes = y_train_cv.shape[1]\n","\n","# Setup GroupKFold for cross-validation within the training dataset\n","n_splits = 5\n","group_kfold = GroupKFold(n_splits=n_splits)\n","best_model_path = 'best_lstm_model.h5'  # Path to save the best model\n","best_accuracy = 0  # Store the best accuracy to compare after each fold\n","\n","# Prepare the ModelCheckpoint callback to save the best model\n","model_checkpoint_callback = ModelCheckpoint(\n","    filepath=best_model_path,\n","    save_weights_only=False,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True\n",")\n","\n","# Running the cross-validation\n","for train_idx, test_idx in group_kfold.split(X_train_cv, y_train_cv, train_groups[train_cv_index]):  # use the correct groups\n","    X_train_fold, X_val_fold = X_train_cv[train_idx], X_train_cv[test_idx]\n","    y_train_fold, y_val_fold = y_train_cv[train_idx], y_train_cv[test_idx]\n","\n","    tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n","\n","    model = create_lstm_model(input_shape, num_classes)\n","\n","    # Train the model with validation fold, checkpointing, and reporting callbacks\n","    history = model.fit(\n","        X_train_fold, y_train_fold,\n","        validation_data=(X_val_fold, y_val_fold),\n","        epochs=100, batch_size=64,\n","        callbacks=[model_checkpoint_callback, report_callback],\n","        verbose=0  # Set verbose to 0 to clean up output\n","    )\n","\n","    # Evaluate the model on the validation fold\n","    score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","    cv_scores.append(score[1])\n","    print(f'Fold accuracy: {score[1]*100:.2f}%')\n","    if score[1]>best_scores:\n","      best_scores=score[1]\n","      best_model=model\n","\n","# Output average accuracy\n","average_accuracy = np.mean(cv_scores)\n","print(f'Average accuracy across folds: {average_accuracy*100:.2f}% ± {np.std(cv_scores)*100:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CKfrSM7vtEj","executionInfo":{"status":"ok","timestamp":1713889197914,"user_tz":240,"elapsed":160154,"user":{"displayName":"Wang","userId":"06822780026042244845"}},"outputId":"f3c6de1e-4c67-46d4-d625-a7b61cb19be1"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 0.7170, Accuracy = 0.4911, Val Loss = 0.6994, Val Accuracy = 0.3750\n","Epoch 40: Loss = 0.6993, Accuracy = 0.4821, Val Loss = 0.7094, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.6949, Accuracy = 0.5000, Val Loss = 0.7012, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.6915, Accuracy = 0.5357, Val Loss = 0.7025, Val Accuracy = 0.3750\n","Epoch 100: Loss = 0.6981, Accuracy = 0.4464, Val Loss = 0.6925, Val Accuracy = 0.6250\n","Fold accuracy: 62.50%\n","Epoch 20: Loss = 0.6991, Accuracy = 0.5357, Val Loss = 0.6945, Val Accuracy = 0.4375\n","Epoch 40: Loss = 0.7097, Accuracy = 0.4911, Val Loss = 0.6891, Val Accuracy = 0.5625\n","Epoch 60: Loss = 0.7176, Accuracy = 0.4643, Val Loss = 0.6986, Val Accuracy = 0.4375\n","Epoch 80: Loss = 0.6980, Accuracy = 0.5089, Val Loss = 0.7073, Val Accuracy = 0.4375\n","Epoch 100: Loss = 0.6943, Accuracy = 0.5179, Val Loss = 0.7147, Val Accuracy = 0.4375\n","Fold accuracy: 43.75%\n","Epoch 20: Loss = 0.7092, Accuracy = 0.5089, Val Loss = 0.6933, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7191, Accuracy = 0.4911, Val Loss = 0.6952, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.7025, Accuracy = 0.4821, Val Loss = 0.6941, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6878, Accuracy = 0.5000, Val Loss = 0.7183, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.6846, Accuracy = 0.5000, Val Loss = 0.7314, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.6925, Accuracy = 0.5417, Val Loss = 0.6953, Val Accuracy = 0.5000\n","Epoch 40: Loss = 0.7029, Accuracy = 0.5917, Val Loss = 0.6991, Val Accuracy = 0.5000\n","Epoch 60: Loss = 0.7136, Accuracy = 0.4917, Val Loss = 0.7054, Val Accuracy = 0.5000\n","Epoch 80: Loss = 0.6951, Accuracy = 0.5917, Val Loss = 0.7096, Val Accuracy = 0.5000\n","Epoch 100: Loss = 0.7048, Accuracy = 0.4250, Val Loss = 0.7198, Val Accuracy = 0.5000\n","Fold accuracy: 50.00%\n","Epoch 20: Loss = 0.7181, Accuracy = 0.5500, Val Loss = 0.6790, Val Accuracy = 0.6667\n","Epoch 40: Loss = 0.7118, Accuracy = 0.5333, Val Loss = 0.7026, Val Accuracy = 0.3750\n","Epoch 60: Loss = 0.6852, Accuracy = 0.5500, Val Loss = 0.7217, Val Accuracy = 0.3750\n","Epoch 80: Loss = 0.7009, Accuracy = 0.5583, Val Loss = 0.7367, Val Accuracy = 0.3333\n","Epoch 100: Loss = 0.6857, Accuracy = 0.5583, Val Loss = 0.7814, Val Accuracy = 0.3333\n","Fold accuracy: 33.33%\n","Average accuracy across folds: 47.92% ± 9.50%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Vwh4fjxsvtcj"},"execution_count":null,"outputs":[]}]}